{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacinthes/slovene-nli-benchmark/blob/main/training/slovene_nli_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Training CrossEncoders**\n",
        "This is the script used to train:\n",
        "\n",
        "*   https://huggingface.co/jacinthes/cross-encoder-sloberta-si-nli-snli-mnli\n",
        "*   https://huggingface.co/jacinthes/cross-encoder-sloberta-si-nli\n"
      ],
      "metadata": {
        "id": "ZtTu6KygZMPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the GPU\n",
        "GPU is needed as a hardware accelerator. \n",
        "Enable it with:\n",
        "- Runtime â†’ Change runtime type\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Use `!nvidia-smi` to see which GPU is assigned."
      ],
      "metadata": {
        "id": "typm7fxcY57b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "mdS85iWUZ3Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Requirements and imports"
      ],
      "metadata": {
        "id": "n45jEFubBkZJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qACyF64kTHFT"
      },
      "outputs": [],
      "source": [
        "pip install datasets sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import LoggingHandler, util\n",
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "from sentence_transformers.cross_encoder.evaluation import CESoftmaxAccuracyEvaluator\n",
        "from sentence_transformers.readers import InputExample\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import os\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import math\n",
        "\n",
        "# Added accuracy info printing after every epoch.\n",
        "class NLICrossEncoder(CrossEncoder):\n",
        "    def _eval_during_training(self, evaluator, output_path, save_best_model, epoch, steps, callback):\n",
        "        \"\"\"Runs evaluation during the training\"\"\"\n",
        "        if evaluator is not None:\n",
        "            score = evaluator(self, output_path=output_path, epoch=epoch, steps=steps)\n",
        "            print(f'Accuracy after epoch: {epoch}: {score}')\n",
        "            if callback is not None:\n",
        "                callback(score, epoch, steps)\n",
        "            if score > self.best_score:\n",
        "                self.best_score = score\n",
        "                if save_best_model:\n",
        "                    self.save(output_path)\n",
        "\n",
        "# Print debug information to stdout\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "                    level=logging.INFO,\n",
        "                    handlers=[LoggingHandler()])\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "KPJ_D1iBT3Mw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing the train and validation sets"
      ],
      "metadata": {
        "id": "QrTjNx4eByfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_mnli_snli = load_dataset('jacinthes/slovene_mnli_snli')\n",
        "dataset_si_nli = load_dataset('cjvt/si_nli')\n",
        "mnli_snli_train = dataset_mnli_snli['train'].to_pandas()\n",
        "si_nli_train = dataset_si_nli['train'].to_pandas()\n",
        "train_df = pd.concat([mnli_snli_train[[\"premise\", \"hypothesis\", 'label']], si_nli_train[[\"premise\", \"hypothesis\", 'label']]], ignore_index=True)"
      ],
      "metadata": {
        "id": "GCj2RJtjT5V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "For the purpose of maximizing the Slovene NLI benchmark score, only si_nli dev set is used to determine the best model. \n",
        "To include the snli & mnli dev sets, uncomment the first and the third line and comment the fourth one.\n",
        "'''\n",
        "#mnli_snli_dev = dataset_mnli_snli['dev'].to_pandas()\n",
        "si_nli_dev = dataset_si_nli['validation'].to_pandas()\n",
        "#dev_df = pd.concat([mnli_snli_dev[[\"premise\", \"hypothesis\", 'label']], si_nli_dev[[\"premise\", \"hypothesis\", 'label']]], ignore_index=True)\n",
        "dev_df = si_nli_dev"
      ],
      "metadata": {
        "id": "lATt5lPWUKyo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label2int = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
        "train_samples = []\n",
        "dev_samples = []\n",
        "for _, row in train_df.iterrows():\n",
        "  label_id = label2int[row['label']]\n",
        "  train_samples.append(InputExample(texts=[row['premise'], row['hypothesis']], label=label_id))\n",
        "\n",
        "for _, row in dev_df.iterrows():\n",
        "  label_id = label2int[row['label']]\n",
        "  dev_samples.append(InputExample(texts=[row['premise'], row['hypothesis']], label=label_id))\n",
        "\n",
        "print(f'Number of training samples: {len(train_samples)}\\nNumber of validation samples: {len(dev_samples)}')"
      ],
      "metadata": {
        "id": "yOYTKz1DUZxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameters"
      ],
      "metadata": {
        "id": "GmcGNUCnC8-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size = 64\n",
        "num_epochs = 8\n",
        "#loss_fct = nn.CrossEntropyLoss() -> default in the CrossEncoder class if num_labels > 2\n",
        "#learning_rate = 2e-5 -> default in the CrossEncoders\n",
        "#optimizer = torch.optim.AdamW -> default in the CrossEncoder class\n",
        "warmup_steps_ratio = 0.1 # % of training steps as warmup\n",
        "weight_decay = 0.002\n",
        "max_sequence_length = 102"
      ],
      "metadata": {
        "id": "abduxqOdDB_G"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "OE5O-3yuEO3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = 'output/training_si-nli-snli-mnli-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "# sloberta is used as the base model\n",
        "model = NLICrossEncoder('EMBEDDIA/sloberta', num_labels=3, max_length=max_sequence_length)\n",
        "\n",
        "# We wrap train_samples, which is a list of InputExample, in a pytorch DataLoader\n",
        "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
        "\n",
        "# During training, use CESoftmaxAccuracyEvaluator to measure the accuracy on the dev set.\n",
        "evaluator = CESoftmaxAccuracyEvaluator.from_input_examples(dev_samples, name='AllNLI-dev')\n",
        "\n",
        "\n",
        "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * warmup_steps_ratio)\n",
        "logger.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataloader=train_dataloader,\n",
        "          evaluator=evaluator,\n",
        "          epochs=num_epochs, \n",
        "          weight_decay=weight_decay,\n",
        "          warmup_steps=warmup_steps,\n",
        "          output_path=model_save_path)"
      ],
      "metadata": {
        "id": "ZodUyWQLU4N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Accuracy, recall and F1 on the dev set"
      ],
      "metadata": {
        "id": "NGKp7aAOFZvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for _, row in si_nli_dev.iterrows():\n",
        "  y_pred.append(model.predict([row['premise'], row['hypothesis']]).argmax())\n",
        "  y_true.append(label2int[row['label']])\n",
        "\n",
        "target_names = ['entailment', 'neutral', 'contradiction']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "EhBulmwUFhOC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}